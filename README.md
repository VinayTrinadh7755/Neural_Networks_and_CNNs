# Neural_Networks_and_CNNs


A PyTorch-based deep learning project implementing feed-forward NNs, CNNs, VGG-13, and ResNet-34â€”and exploring optimization, regularization, and interpretability without using pre-built architectures.

## ğŸ” Project Overview

This assignment is organized into four core parts plus a bonus:

- **Basic NN** â€“ Basic Fully Connected NN on tabular data  
- **Optimizing NN** â€“ Hyperparameter Tuning & Optimization Techniques  
- **Convolutional NN** â€“ Convolutional Neural Network on a 36-class image dataset  
- **VGG-13** â€“ Custom VGG-13 (â€œVersion Bâ€) for 36-class classification  
- **REsnet-34 and Interpretability**   â€“ ResNet-34 from scratch & feature/activation/weight visualization  

Each part is delivered as a self-contained Jupyter notebook with:
- Preprocessing code  
- Model definition  
- Training & evaluation loops  
- Plots (accuracy, loss, ROC, confusion matrices)  
- Saved weights (.pt files)  

## ğŸ¯ Why This Matters

Deep learning proficiency requires you to:
- Design and implement custom architectures (NN, CNN, VGG, ResNet)  
- Master PyTorch basics: nn.Module, optimizers, loss functions  
- Apply regularization (dropout, batch norm), learning-rate schedules, early stopping  
- Tune hyperparameters methodically and interpret metrics (accuracy, precision/recall/F1, AUC)  
- Visualize model predictions and gain insights via ROC curves and confusion matrices  
- Build reproducible experiments with notebooks, pickled weights, and clear reporting  

## ğŸ—‚ Table of Contents

1. [Quick Start](#quick-start)  
2. [Notebooks Reference](#notebooks-reference)  
3. [Architecture & Design](#architecture--design)  
4. [Key Features](#key-features)  
5. [Project Structure](#project-structure)  
6. [Usage Examples](#usage-examples)  
7. [Tech Stack](#tech-stack)  
8. [Future Enhancements](#future-enhancements)  
9. [Contributing](#contributing)  
10. [License](#license)  
11. [Authors & Contact](#authors--contact)  

---

## ğŸ”§ Quick Start

```bash
# 1. Clone repository
git clone https://github.com/YourOrg/cse574-assignment2-ml.git
cd cse574-assignment2-ml

# 2. Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Launch Jupyter
jupyter notebook
```

Run notebooks in sequence:

- BasicNN.ipynb
- OptimizingNN.ipynb
- ConvolutionalNN.ipynb
- VGG-13.ipynb
- Resnet-34.ipynb

## ğŸ“œ Notebooks Reference

| Notebook                          | Description                                        | Output Artifacts                                  |
|-----------------------------------|----------------------------------------------------|---------------------------------------------------|
| Basic_NN.ipynb                    | Basic NN: data loading, preprocessing, model build | BasicNN_best_model.pt, metrics & plots            |
| Optimizing_NN.ipynb               | NN optimization: dropout, activations, schedulers  | OptimizingNN_best_model.pt, tuning tables & plots |
| COnvolutional_NN.ipynb            | Convolutional_NN: conv layers, batch norm, pooling | Convolutional_NN_weights.pt, metrics &plots       |
| VGG-13.ipynb                      | VGG-13: deep conv blocks + FC layers               | vgg13_weights.pt, metrics & plots                 |
| Resnet-34.ipynb                   | ResNet-34 from scratch + interpretability hooks    | resnet34_weights.pt, saliency maps                |

## ğŸ— Architecture & Design

**Data Preprocessing**  
â€“ Tabular: non-numeric cleanup, imputation, scaling, oversampling  
â€“ Images: torchvision.transforms for resizing, normalization  

**Model Classes**  
â€“ FeedForwardNet â€“ Modular MLP with configurable layers  
â€“ ConvNet â€“ 3-layer CNN with BatchNorm + Dropout  
â€“ VGG13 â€“ Custom Version B architecture adapted for grayscale  
â€“ ResNet34 â€“ Residual blocks with identity shortcuts  

**Training Pipeline**  
- Load & preprocess data  
- Instantiate model & move to device  
- Define loss (BCE or Cross-Entropy) & optimizer (SGD/Adam)  
- Training loop with optional LR scheduler & early stopping  
- Evaluate on validation & test sets  
- Save best weights with torch.save()  

**Evaluation**  
- Accuracy, Precision/Recall/F1, ROC AUC  
- Confusion matrix heatmaps & per-epoch loss/accuracy plots  

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   preprocess    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   train    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ raw data â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ cleaned  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ models   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”‘ Key Features

**Part I â€“ Basic NN**
- Clean non-numeric entries, impute means, scale & oversample
- 7â†’64â†’64â†’1 network; ReLU + Dropout; Sigmoid output

**Part II â€“ Optimization**
- Dropout tuning & deeper architectures
- Activation experiments (Leaky ReLU, ELU)
- BatchNorm, EarlyStopping, LR Schedulers, K-Fold CV

**Part III â€“ CNN**
- 3Ã—(Convâ†’BatchNormâ†’ReLUâ†’MaxPool) + FC layers
- Achieved âˆ¼91% accuracy on 36-class 28Ã—28 dataset

**Part IV â€“ VGG-13**
- Five conv blocks (2Ã—3Ã—3 conv + pool) + three FC layers
- Adapted for 1-channel input & 36-way output; âˆ¼91.5% accuracy

**ResNet-34 & Interpretability**
- Built residual blocks with skip connections
- Visualized feature maps, activations, and convolutional kernels

## ğŸ“ Project Structure

```
cse574-assignment2-ml/
â”œâ”€â”€ data/                       # Raw & preprocessed datasets
â”‚   â”œâ”€â”€ tabular/                # dataset.csv
â”‚   â”œâ”€â”€ cnn_dataset/            # 36-class images
â”‚   â””â”€â”€ flower_data/                  # Flower images for ResNet task
â”œâ”€â”€ notebooks/                  # Jupyter notebooks for each part
â”‚   â”œâ”€â”€ Basic_NN.ipynb
â”‚   â”œâ”€â”€ Optimizing_NN.ipynb
â”‚   â”œâ”€â”€ Convolutional_NN.ipynb
â”‚   â”œâ”€â”€ VGG-13.ipynb
â”‚   â””â”€â”€ Resnet-34.ipynb
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
## ğŸš€ Usage Examples

```python
# Part I: Train basic NN
from feedforward import FeedForwardNet
model = FeedForwardNet(input_size=7, hidden_sizes=[64,64], dropout=0.5)
model.to(device)
# â€¦ load data into DataLoader â€¦
train(model, train_loader, val_loader,
      criterion='BCE', optimizer='Adam', lr=1e-3,
      epochs=10, save_path='models/part1_best_model.pt')
```

```python
# Part III: CNN evaluation
from cnn import ConvNet
cnn = ConvNet(num_classes=36).to(device)
cnn.load_state_dict(torch.load('models/part3_cnn_weights.pt'))
test_acc, test_metrics = evaluate(cnn, test_loader)
print(f"Test Accuracy: {test_acc:.2%}")
```

## ğŸ›  Tech Stack

- **Language**: Python 3.8+  
- **Framework**: PyTorch  
- **Data & Preprocessing**: pandas, NumPy, scikit-learn  
- **Visualization**: Matplotlib, Seaborn, Torchinfo  
- **Experiment Tracking**: Pickle, torch.save/load  
- **Notebook**: Jupyter  

## ğŸŒ± Future Enhancements

- Automated hyperparameter search (Optuna, Ray Tune)  
- Mini-batch & distributed training for large datasets  
- Grad-CAM visualization for CNN interpretability  
- Ensemble of CNN + VGG + ResNet for marginal gains  
- CI/CD pipeline (GitHub Actions) for reproducible training  

## ğŸ¤ Contributing

1. Fork the repo  
2. Create a feature branch  
```bash
git checkout -b feature/awesome-enhancement
```
3. Commit your changes  
```bash
git commit -m "Add awesome feature"
```
4. Push & open a Pull Request  

Feedback and improvements are always welcome!

## ğŸ“œ License

This project is licensed under the MIT License. See LICENSE for details.

## ğŸ‘¥ Authors & Contact

**Vinay Trinadh Naraharisetty** 
[GitHub](https://github.com/VinayTrinadh7755)  
[LinkedIn](www.linkedin.com/in/vinay-trinadh-naraharisetty)

Thank you for exploring our deep learning implementations!